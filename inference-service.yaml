apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llava-multimodal
  namespace: multimodal-demo
  annotations:
    opendatahub.io/connections: pvc-multimodal-connection
    opendatahub.io/genai-use-case: vision-language, image understanding, multimodal chat
    opendatahub.io/hardware-profile-name: nvidia-gpu-l40
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/model-type: generative
    openshift.io/display-name: LLaVA Multimodal
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 1
    minReplicas: 1
    model:
      args:
      - --model
      - /mnt/models
      - --tensor-parallel-size
      - "1"
      - --gpu-memory-utilization
      - "0.9"
      - --max-model-len
      - "2048"
      - --dtype
      - auto
      - --served-model-name
      - llava-multimodal
      - --max-num-seqs
      - "256"
      modelFormat:
        name: vLLM
      name: ""
      resources:
        limits:
          cpu: "8"
          memory: 32Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "4"
          memory: 16Gi
          nvidia.com/gpu: "1"
      runtime: llava-multimodal
      storageUri: pvc://pvc-multimodal
    # nodeSelector retiré pour permettre le scheduling sur n'importe quel nœud GPU
    # nodeSelector:
    #   topology.kubernetes.io/zone: eu-central-1a
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Equal
      value: NVIDIA-L40-PRIVATE

